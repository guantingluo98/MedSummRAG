{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/pytorch_lightning/utilities/distributed.py:23: UserWarning: Checkpoint directory /data/luo/hqs/model/ exists and is not empty with save_top_k != 0.All files in this directory will be deleted when a checkpoint is saved!\n",
      "  warnings.warn(*args, **kwargs)\n",
      "INFO:nlp.utils.file_utils:PyTorch version 1.10.0+cu111 available.\n",
      "INFO:faiss.loader:Loading faiss with AVX2 support.\n",
      "INFO:faiss.loader:Successfully loaded faiss with AVX2 support.\n",
      "INFO:datasets:PyTorch version 1.10.0+cu111 available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_VISIBLE_DEVICES: 0\n",
      "CUDA is available with 1 GPU(s)!\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "import torch\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModel, BartForConditionalGeneration, BartTokenizer\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "import HQS_test_biobart\n",
    "import argparse\n",
    "import time\n",
    "from rank_bm25 import BM25Okapi\n",
    "import rouge155\n",
    "import json\n",
    "import rag_retrieval\n",
    "from HQS_RAG_train_biobart_end2end_real import BARTTuner\n",
    "num_gpus = torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"biobartcheckpoint\"\n",
    "test_list = HQS_test_biobart.getDatalist(\"test_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_dict = dict(\n",
    "    output_dir=\"...\",\n",
    "    model_name_or_path='GanjinZero/biobart-large',\n",
    "    tokenizer_name_or_path='GanjinZero/biobart-large',\n",
    "    max_input_length=1024,\n",
    "    max_output_length=50,\n",
    "    freeze_encoder=False,\n",
    "    freeze_embeds=False,\n",
    "    num_train_epochs=20,\n",
    "    eval_batch_size=8,\n",
    "    learning_rate=0.00006,\n",
    "    weight_decay=0.0,\n",
    "    adam_epsilon=1e-7,\n",
    "    warmup_steps=0,\n",
    "    train_batch_size=2,\n",
    "    gradient_accumulation_steps=16,\n",
    "    n_gpu=-1,\n",
    "    resume_from_checkpoint=None, \n",
    "    val_check_interval = 0.05, \n",
    "    n_val=1000,\n",
    "    n_train=-1,\n",
    "    n_test=-1,\n",
    "    early_stop_callback=False,\n",
    "    fp_16=True,\n",
    "    opt_level='O1',\n",
    "    max_grad_norm=1.0,\n",
    "    seed=42,\n",
    "    tau=1.0,\n",
    "    lambda_CL=1.0,\n",
    "    lambda_medical=0.0021,\n",
    "    lambda_negation=0.0021,\n",
    ")\n",
    "args = argparse.Namespace(**args_dict)\n",
    "\n",
    "bge_model_name = \"BAAI/bge-m3\"\n",
    "bge_tokenizer = AutoTokenizer.from_pretrained(bge_model_name)\n",
    "bge_model = AutoModel.from_pretrained(bge_model_name).to(device)\n",
    "\n",
    "bart_model_name = \"GanjinZero/biobart-large\"\n",
    "bart_tokenizer = BartTokenizer.from_pretrained(bart_model_name)\n",
    "bart_model = BartForConditionalGeneration.from_pretrained(bart_model_name)\n",
    "\n",
    "class BARTTuner_e2e(pl.LightningModule):\n",
    "    def __init__(self,batchsize, model, tokenizer, max_input_length, top_n, kb, p_tokenizer, pass_encoder):\n",
    "        super(BARTTuner_e2e, self).__init__()\n",
    "        self.batch_size = batchsize\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_length = max_input_length\n",
    "        self.top_n = top_n\n",
    "        self.knowledge_base = kb\n",
    "        self.passage_tokenizer = p_tokenizer\n",
    "        self.passage_encoder = pass_encoder\n",
    "        self.bm25 = None\n",
    "        self.retrieval = {}\n",
    "    \n",
    "    def bm25_retrieve_batch(self, queries, knowledge_base, top_n):\n",
    "        tokenized_corpus = [doc.split() for doc in knowledge_base]\n",
    "        bm25 = BM25Okapi(tokenized_corpus)\n",
    "        top_n_indices_list = []\n",
    "        top_n_scores_list = []\n",
    "        num_batches = len(queries) // self.batch_size\n",
    "        if len(queries) % self.batch_size != 0:\n",
    "            num_batches += 1\n",
    "\n",
    "        for batch_idx in range(num_batches):\n",
    "            start_idx = batch_idx * self.batch_size\n",
    "            end_idx = min((batch_idx + 1) * self.batch_size, len(queries))\n",
    "\n",
    "            batch_queries = queries[start_idx:end_idx]\n",
    "            tokenized_queries = [query.split() for query in batch_queries]\n",
    "\n",
    "            batch_doc_scores = []\n",
    "            for tokenized_query in tokenized_queries:\n",
    "                doc_scores = bm25.get_scores(tokenized_query)\n",
    "                batch_doc_scores.append(doc_scores)\n",
    "\n",
    "            batch_top_n_indices = []\n",
    "            batch_top_n_scores = []\n",
    "            for doc_scores in batch_doc_scores:\n",
    "                top_n_indices = np.argsort(doc_scores)[::-1][:top_n]\n",
    "                top_n_scores = [doc_scores[i] for i in top_n_indices]\n",
    "                batch_top_n_indices.append(top_n_indices.tolist())\n",
    "                batch_top_n_scores.append(top_n_scores)\n",
    "\n",
    "            top_n_indices_list.extend(batch_top_n_indices)\n",
    "            top_n_scores_list.extend(batch_top_n_scores)\n",
    "\n",
    "        return top_n_indices_list, top_n_scores_list\n",
    "    \n",
    "    def get_embeddings(self, text, model, mode):\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        outputs = model(**inputs)\n",
    "        if mode == \"mean\":\n",
    "            embeddings = outputs.last_hidden_state.mean(dim=1).squeeze()\n",
    "        elif mode == \"cls\":\n",
    "            embeddings = outputs.last_hidden_state[:, 0, :].squeeze()\n",
    "        elif mode == \"wopool\":\n",
    "            embeddings = outputs.encoder_last_hidden_state.squeeze()\n",
    "        else:\n",
    "            raise ValueError(\"Invalid mode. Choose between 'mean' and 'cls' and 'wopool'.\")\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "    def pad_to_length(self, tensor, max_length):\n",
    "        pad_size = max_length - tensor.shape[1]\n",
    "        padding = (0, 0, 0, pad_size)\n",
    "        return torch.nn.functional.pad(tensor, padding)\n",
    "\n",
    "    def bm25_retrieve(self, query, knowledge_base, top_n):\n",
    "        if query in self.retrieval.keys():\n",
    "            return self.retrieval[query]\n",
    "        tokenized_query = query.split()\n",
    "        doc_scores = self.bm25.get_scores(tokenized_query)\n",
    "        top_n_indices = np.argsort(doc_scores)[::-1][:top_n]\n",
    "        top_n_knowledge = [knowledge_base[i] for i in top_n_indices]\n",
    "        top_n_scores = [doc_scores[i] for i in top_n_indices]\n",
    "        top_n_indices_list = top_n_indices.tolist()\n",
    "        self.retrieval[query] = top_n_indices_list, top_n_scores\n",
    "        return top_n_indices_list, top_n_scores\n",
    "\n",
    "    def forward(self, input_embeds, attention_mask=None,\n",
    "                decoder_attention_mask=None,\n",
    "                lm_labels=None):\n",
    "        outputs = self.model(\n",
    "            inputs_embeds=input_embeds,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_attention_mask=decoder_attention_mask,\n",
    "            labels=lm_labels\n",
    "        )\n",
    "        return outputs\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        idx_batch = batch['index']\n",
    "        query_batch = batch['query']\n",
    "        doc_batch = batch['doc']\n",
    "        target_batch = batch['target']\n",
    "        kb = self.knowledge_base[\"validation\"]\n",
    "        input_embeds_list=[]\n",
    "        mask_list=[]\n",
    "        batch_l1_norm = torch.tensor(0.0, dtype=torch.float32, device=self.model.device)\n",
    "        for q, doc in zip(query_batch, doc_batch):\n",
    "            input_ids = self.tokenizer(q, return_tensors=\"pt\", padding=True, truncation=True).to(self.model.device)\n",
    "            input_embeds = self.model.model.shared(input_ids.input_ids)\n",
    "            question_length = input_embeds.shape[1]\n",
    "            knowledge_embeds_list = [self.get_embeddings(doc, self.model, 'wopool')]\n",
    "            knowledge_embeds_for_expanded = torch.stack(knowledge_embeds_list, dim=0)\n",
    "            knowledge_embeds_for_expanded = knowledge_embeds_for_expanded.expand(input_embeds.size(0), -1, -1)\n",
    "            input_embeds = torch.cat([input_embeds, knowledge_embeds_for_expanded], dim=1)\n",
    "            current_length = input_embeds.shape[1]\n",
    "            if current_length>args.max_input_length:\n",
    "                print(\"current_length: \", current_length)\n",
    "            concatenate_length = input_embeds.shape[1]\n",
    "            input_embeds = self.pad_to_length(input_embeds, self.max_input_length)\n",
    "            input_embeds_list.append(input_embeds.squeeze(0))\n",
    "            concat_attention_mask = torch.zeros(self.max_input_length, dtype=torch.long)\n",
    "            concat_attention_mask[:concatenate_length] = 1\n",
    "            mask_list.append(concat_attention_mask)\n",
    "        input_embeds_batch=torch.stack(input_embeds_list).to(self.model.device)\n",
    "        mask_batch=torch.stack(mask_list).to(self.model.device)\n",
    "\n",
    "        outputs = self.forward(\n",
    "            input_embeds=input_embeds_batch,\n",
    "            decoder_attention_mask=batch['decoder_attention_mask'],\n",
    "            lm_labels=batch['labels']\n",
    "        )\n",
    "        loss = outputs[0]\n",
    "        tensorboard_logs = {\"train_loss\": loss}\n",
    "        return {\"loss\": loss, \"log\": tensorboard_logs}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        idx_batch = batch['index']\n",
    "        query_batch = batch['query']\n",
    "        doc_batch = batch['doc']\n",
    "        target_batch = batch['target']\n",
    "        kb = self.knowledge_base[\"validation\"]\n",
    "        input_embeds_list=[]\n",
    "        mask_list=[]\n",
    "        batch_l1_norm = torch.tensor(0.0, dtype=torch.float32, device=self.model.device)\n",
    "        for q, doc in zip(query_batch, doc_batch):\n",
    "            input_ids = self.tokenizer(q, return_tensors=\"pt\", padding=True, truncation=True).to(self.model.device)\n",
    "            input_embeds = self.model.model.shared(input_ids.input_ids)\n",
    "            question_length = input_embeds.shape[1]\n",
    "            knowledge_embeds_list = [self.get_embeddings(doc, self.model, 'wopool')]\n",
    "            knowledge_embeds_for_expanded = torch.stack(knowledge_embeds_list, dim=0)\n",
    "            knowledge_embeds_for_expanded = knowledge_embeds_for_expanded.expand(input_embeds.size(0), -1, -1)\n",
    "            input_embeds = torch.cat([input_embeds, knowledge_embeds_for_expanded], dim=1)\n",
    "            concatenate_length = input_embeds.shape[1]\n",
    "            input_embeds = self.pad_to_length(input_embeds, self.max_input_length)\n",
    "            input_embeds_list.append(input_embeds.squeeze(0))\n",
    "            concat_attention_mask = torch.zeros(self.max_input_length, dtype=torch.long)\n",
    "            concat_attention_mask[:concatenate_length] = 1\n",
    "            mask_list.append(concat_attention_mask)\n",
    "        input_embeds_batch=torch.stack(input_embeds_list).to(self.model.device)\n",
    "        mask_batch=torch.stack(mask_list).to(self.model.device)\n",
    "        \n",
    "        target=batch[\"target\"]\n",
    "        outs = self.model.generate(\n",
    "                inputs_embeds=input_embeds_batch,\n",
    "                max_new_tokens=args.max_output_length,\n",
    "                use_cache=True,\n",
    "                num_beams=5,\n",
    "                repetition_penalty=1, \n",
    "                length_penalty=2, \n",
    "                early_stopping=True,\n",
    "                )\n",
    "        preds = [self.tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in outs]\n",
    "        scores = rouge155.rouge_eval(target, preds)\n",
    "        loss = scores['rouge_l_f_score']\n",
    "        tensorboard_logs = {\"val_rouge\": loss}\n",
    "        return {\"val_rouge\": loss, \"log\": tensorboard_logs}\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_val_loss = sum(x[\"val_rouge\"] for x in outputs) / len(outputs)\n",
    "        tensorboard_logs = {'avg_val_rouge': avg_val_loss}\n",
    "        return {'avg_val_rouge': avg_val_loss, \"log\": tensorboard_logs, 'progress_bar': tensorboard_logs}\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(train_dataset, batch_size=self.batch_size,\n",
    "                          num_workers=4)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(validation_dataset,\n",
    "                          batch_size=self.batch_size,\n",
    "                          num_workers=4)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "        return optimizer\n",
    "\n",
    "p_tokenizer = AutoTokenizer.from_pretrained(\"GanjinZero/biobart-large\")\n",
    "p_encoder = AutoModel.from_pretrained(\"GanjinZero/biobart-large\")\n",
    "model = BARTTuner_e2e.load_from_checkpoint( \n",
    "    checkpoint_path,\n",
    "    batchsize=4, \n",
    "    model=bart_model, \n",
    "    tokenizer=bart_tokenizer,\n",
    "    max_input_length=1024,\n",
    "    top_n=5,\n",
    "    kb=[\"\"],\n",
    "    p_tokenizer=p_tokenizer,\n",
    "    pass_encoder=p_encoder)\n",
    "passage_tokenizer = model.passage_tokenizer\n",
    "passage_encoder = model.passage_encoder\n",
    "passage_encoder = passage_encoder.to(device)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "RESULTS=[]\n",
    "output_path = \"checkpointsavepath\"+checkpoint_path.split(\".ckpt\")[0].split(\"/\")[-1]+\".txt\"\n",
    "def get_embeddings(text, tokenizer, model):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze()\n",
    "\n",
    "def get_embeddings_cls(text, tokenizer, model):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].squeeze()\n",
    "\n",
    "def bge_retrieve(top_n, input_text, db, db_embeds):\n",
    "    instruction = input_text\n",
    "    if db_embeds==None:\n",
    "        knowledge_embeddings = torch.stack([get_embeddings(doc, bge_tokenizer, bge_model) for doc in db]).to(device)\n",
    "    else:\n",
    "        knowledge_embeddings = db_embeds\n",
    "    instruction_embedding = get_embeddings(instruction, bge_tokenizer, bge_model).to(device)\n",
    "    similarities = torch.matmul(knowledge_embeddings, instruction_embedding) / (\n",
    "        torch.norm(knowledge_embeddings, dim=1) * torch.norm(instruction_embedding)\n",
    "    )\n",
    "    top_n_indices = torch.argsort(similarities, descending=True)[:top_n]\n",
    "    top_n_knowledge = [db[i] for i in top_n_indices]\n",
    "    top_n_scores = [similarities[i].item() for i in top_n_indices]\n",
    "    top_n_indices_list = top_n_indices.cpu().numpy().tolist()\n",
    "    tmp = {\"idxs\":top_n_indices_list, \"scores\":top_n_scores}\n",
    "    RESULTS.append(tmp)\n",
    "    return top_n_indices_list, top_n_scores\n",
    "\n",
    "def bm25_retrieve(query, knowledge_base, top_n):\n",
    "    tokenized_corpus = [doc.split() for doc in knowledge_base]\n",
    "    bm25 = BM25Okapi(tokenized_corpus)\n",
    "    tokenized_query = query.split()\n",
    "    doc_scores = bm25.get_scores(tokenized_query)\n",
    "    top_n_indices = np.argsort(doc_scores)[::-1][:top_n]\n",
    "    top_n_knowledge = [knowledge_base[i] for i in top_n_indices]\n",
    "    top_n_scores = [doc_scores[i] for i in top_n_indices]\n",
    "    top_n_indices_list = top_n_indices.tolist()\n",
    "    tmp = {\"idxs\":top_n_indices_list, \"scores\":top_n_scores}\n",
    "    RESULTS.append(tmp)\n",
    "    return top_n_indices_list, top_n_scores\n",
    "\n",
    "def retrieve_definition(query, knowledge_base):\n",
    "    for entry in knowledge_base:\n",
    "        if entry[\"question\"] == query:\n",
    "            return entry.get(\"q_definition\", \" \")\n",
    "    return \" \"\n",
    "\n",
    "def bge_reranker(query, knowledge_base, top_re, top_n):\n",
    "    top_re_indices_list, top_re_scores = bm25_retrieve(query, knowledge_base, top_re)\n",
    "    kb_rerank = [knowledge_base[idx] for idx in top_re_indices_list]\n",
    "    top_n_indices_list, top_n_scores = bge_retrieve(top_n, query, kb_rerank)\n",
    "    return kb_rerank, top_n_indices_list, top_n_scores\n",
    "NULL_COUNT=0\n",
    "def inference(model, top_n, input_text, doc, retriever):\n",
    "    bart_input_text = input_text\n",
    "    bart_inputs = bart_tokenizer(bart_input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    bart_inputs = bart_inputs.to(device)\n",
    "    input_embeds = bart_model.model.shared(bart_inputs.input_ids)\n",
    "    knowledge_embeds_list = model.get_embeddings([doc], model.model, 'wopool')\n",
    "    knowledge_embeds_for_bart = torch.stack([knowledge_embeds_list], dim=0)\n",
    "    input_length = bart_inputs.input_ids.shape[-1]\n",
    "    knowledge_embeds_for_expanded = knowledge_embeds_for_bart.expand(input_embeds.size(0), -1, -1)\n",
    "    if knowledge_embeds_for_expanded.shape[1]<4:\n",
    "        NULL_COUNT+=1\n",
    "    input_embeds = torch.cat([input_embeds, knowledge_embeds_for_expanded], dim=1)\n",
    "    if input_embeds.shape[1] > args.max_input_length:\n",
    "        input_embeds = input_embeds[:, :args.max_input_length, :]\n",
    "    concatenate_length = input_embeds.shape[1]\n",
    "    concat_attention_mask = torch.ones(concatenate_length, dtype=torch.long)\n",
    "    concat_attention_mask = concat_attention_mask.unsqueeze(dim=0).to(device)\n",
    "    input_embeds = input_embeds.to(device)\n",
    "    outs = model.model.generate(\n",
    "                inputs_embeds=input_embeds,\n",
    "                attention_mask=concat_attention_mask,\n",
    "                max_new_tokens=args.max_output_length,\n",
    "                use_cache=True,\n",
    "                num_beams=5,\n",
    "                repetition_penalty=1, \n",
    "                length_penalty=2, \n",
    "                early_stopping=True,\n",
    "                )\n",
    "    sentence = bart_tokenizer.decode([int(id) for id in outs[0]], skip_special_tokens=True)\n",
    "    return sentence\n",
    "\n",
    "def infer_dataset(dataset, model, top_n, db, retriever, des):\n",
    "    res = []\n",
    "    db_embeds = 0\n",
    "    for i, item in enumerate(dataset):\n",
    "        instruction = item[\"question\"]\n",
    "        doc = item[\"retrieval\"][0][\"doc\"]\n",
    "        sentence = inference(model, top_n, instruction, doc, retriever)\n",
    "        sentence = sentence.replace(\"\\n\", \"\")\n",
    "        res.append(sentence)\n",
    "    with open(des, \"w\")as f:\n",
    "        for item in res:\n",
    "            f.write(item+\"\\n\")\n",
    "\n",
    "def read_dict(path):\n",
    "    with open(path, 'r') as file:\n",
    "        data = file.read()\n",
    "    dictionary = json.loads(data)\n",
    "    return dictionary\n",
    "\n",
    "dataset = \"yahoo\"\n",
    "retriever_list=[\"bm25\", \"bge_rerank\", \"bge\"]\n",
    "retriever = retriever_list[0]\n",
    "targets_dict = {\n",
    "    \"yahoo\":\".../test.target\"}\n",
    "targets_path = targets_dict[dataset]\n",
    "train_list = HQS_test_biobart.getDatalist(\"train_dataset\".format(dataset))\n",
    "knowledge_base_v1 = [item[\"question\"] for item in train_list]\n",
    "\n",
    "def load_datalist(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line.strip()))\n",
    "    return data\n",
    "\n",
    "test_dataset = HQS_test_biobart.MyDataset(test_list, bart_tokenizer, args.max_input_length, args.max_output_length)\n",
    "do_eva = True\n",
    "\n",
    "def read_dict(path):\n",
    "    with open(path, 'r') as file:\n",
    "        data = file.read()\n",
    "    dictionary = json.loads(data)\n",
    "    return dictionary\n",
    "\n",
    "query=train_list[0][\"question\"]\n",
    "top_n_indices_list, top_n_scores = bm25_retrieve(query, knowledge_base_v1, 5)\n",
    "bart_input_text = query\n",
    "bart_inputs = bart_tokenizer(bart_input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "bart_inputs = bart_inputs.to(device)\n",
    "input_embeds = bart_model.model.shared(bart_inputs.input_ids)\n",
    "db=knowledge_base_v1\n",
    "knowledge_embeds_list = [\n",
    "get_embeddings(db[idx], bart_tokenizer, bart_model.model)\n",
    "knowledge_embeds_for_bart = torch.stack(knowledge_embeds_list, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NULL_COUNT=0\n",
    "for i in [0]:\n",
    "    RESULTS=[]\n",
    "    top_n=i+1\n",
    "    decodes_path = \"savepath\".format(dataset, top_n, retriever)\n",
    "    start_time = time.time()\n",
    "    infer_dataset(test_list, model, top_n, kb_test, retriever, decodes_path)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    if do_eva:\n",
    "        print(rouge155.calculate_rouge155_md(targets_path, decodes_path))\n",
    "    for i, item in enumerate(RESULTS):\n",
    "        item['index'] = i\n",
    "    with open('save', 'w', encoding='utf-8') as f:\n",
    "        json.dump(RESULTS, f, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
